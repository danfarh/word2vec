{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Word2Vec with CBOW**"
      ],
      "metadata": {
        "id": "J-7G-OGxuMhi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1htD3uRTqfry"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "elTVD2pVOMFp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Embedding, Lambda, Reshape, Conv1D, MaxPool1D, Dropout\n",
        "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1xCNiunBhTK"
      },
      "source": [
        "## **Read Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "--GFqd9iqyPb"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('data.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4w3NERBvq55R",
        "outputId": "a3447554-fb05-470f-cf75-c57f25efbf25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   1\n",
              "0  ای رستخیز ناگهان، وی رحمت بی منتها\\tای آتشی اف...\n",
              "1  امروز خندان آمدی، مفتاح زندان آمدی\\tبر مستمندا...\n",
              "2  خورشید را حاجب تویی، امید را واجب تویی\\tمطلب ت...\n",
              "3  در سینه ها برخاسته، اندیشه را آراسته\\tهم خویش ...\n",
              "4  ای روح بخش بی بَدَل، وی لذتِ علم و عمل\\tباقی ب..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2cabc2c7-a225-42b5-97b6-dd997a4069a0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ای رستخیز ناگهان، وی رحمت بی منتها\\tای آتشی اف...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>امروز خندان آمدی، مفتاح زندان آمدی\\tبر مستمندا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>خورشید را حاجب تویی، امید را واجب تویی\\tمطلب ت...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>در سینه ها برخاسته، اندیشه را آراسته\\tهم خویش ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ای روح بخش بی بَدَل، وی لذتِ علم و عمل\\tباقی ب...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2cabc2c7-a225-42b5-97b6-dd997a4069a0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2cabc2c7-a225-42b5-97b6-dd997a4069a0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2cabc2c7-a225-42b5-97b6-dd997a4069a0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckG4SpxpBv0v"
      },
      "source": [
        "**Read Stop-Words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j1ET4Envq9WJ"
      },
      "outputs": [],
      "source": [
        "def read_stop_words(filename):\n",
        "  with open(filename) as stopwords_file:\n",
        "    stopwords = stopwords_file.readlines()\n",
        "  stopwords = [line.replace('\\n', '') for line in stopwords] \n",
        "  return stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcvHmY1GrB6m",
        "outputId": "b5fcee14-13c4-4fe0-fced-608558ef3422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1382\n"
          ]
        }
      ],
      "source": [
        "stopwords = read_stop_words('stopwords.txt')\n",
        "print(len(stopwords))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrUF5WHobMYG"
      },
      "source": [
        "**hazm library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51aGKS8zcXBb",
        "outputId": "425dc910-4596-4242-ed97-42e217b01d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 35.8 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 39.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394485 sha256=30e7045f960854309a5a1a108fe10ebf8207d8cee09ece6c116f9a9114b03800\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154476 sha256=b0bfdbcc58536e8edee4fe9047648631e403fee7fbda2e74561760efa623f346\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ],
      "source": [
        "# install hazm library\n",
        "!pip install hazm\n",
        "from hazm import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efCRPjKQbXHH"
      },
      "source": [
        "### **Preprocess the text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nCg2QfwVa_6C"
      },
      "outputs": [],
      "source": [
        "# preprocess the text\n",
        "def text_preprocess(data):\n",
        "  text = [line.replace('\\t', ' ') for line in data.values.flatten()]\n",
        "  text = [line.replace('-', ' ') for line in text]\n",
        "  text = [re.sub(\"\\d+\", \"\", t) for t in text]\n",
        "\n",
        "  word_tokenized = [word_tokenize(t) for t in text]\n",
        "  word_tokenized_filtered = [[w for w in sentence if w not in stopwords] for sentence in word_tokenized]\n",
        "\n",
        "  sentences = [' '.join(sentence) for sentence in word_tokenized_filtered]\n",
        "  sentences = [sentence for sentence in sentences if sentence != '']\n",
        "\n",
        "  return sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BbJ_TpjbrR52"
      },
      "outputs": [],
      "source": [
        "sentences = text_preprocess(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "EN2dI2uITsRP",
        "outputId": "0a80ddc0-7cae-4625-feca-515dd76b63bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'رستخیز رحمت منتها آتشی افروخته بیشه اندیشه'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "sentences[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1buSxp96bxfF"
      },
      "source": [
        "### **get less frequente words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VnVjtr5ldH0h"
      },
      "outputs": [],
      "source": [
        "def get_all_sentences():\n",
        "  all_sentences = ''\n",
        "  sentences = text_preprocess(data)\n",
        "  for sentence in sentences:\n",
        "    all_sentences += sentence\n",
        "    all_sentences += ' '\n",
        "\n",
        "  return all_sentences  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6mjyKRbHeXGB"
      },
      "outputs": [],
      "source": [
        "def get_word_freq(vocabularies):\n",
        "  word_freq = []\n",
        "  for vocab in vocabularies:\n",
        "    word_freq.append(vocabularies.count(vocab))\n",
        "  return word_freq  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Q3BTyonArXoS"
      },
      "outputs": [],
      "source": [
        "# get the words that frequentes less than 2 times in the corpus\n",
        "def get_less_frequente_words():\n",
        "  low_frequency_words = []\n",
        "  all_sentences = get_all_sentences()\n",
        "  vocabularies = all_sentences.split(' ')\n",
        "  word_freq = get_word_freq(vocabularies)\n",
        "\n",
        "  for i in range(len(word_freq)):\n",
        "    if word_freq[i] < 2:\n",
        "      low_frequency_words.append(vocabularies[i])\n",
        "\n",
        "  return low_frequency_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "i758iopdesxx"
      },
      "outputs": [],
      "source": [
        "less_frequente_words = get_less_frequente_words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT3CZ3SK3Obj",
        "outputId": "806e4109-99db-4b50-f88a-ef4e104ad811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6366\n"
          ]
        }
      ],
      "source": [
        "print(len(less_frequente_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebfg1Rvyb-Gv"
      },
      "source": [
        "## **remove less frequente words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vC7K9jAcv0Qz"
      },
      "outputs": [],
      "source": [
        "# remove the words that frequentes less than 2 times in the corpus\n",
        "def remove_less_frequente_words(less_frequente_words, sentences):\n",
        "  sentences_tokenized = [word_tokenize(sentence) for sentence in sentences]\n",
        "  sentences_tokenized_filtered = [[w for w in sentence if w not in less_frequente_words] for sentence in sentences_tokenized]\n",
        "  corpus = [' '.join(sentence) for sentence in sentences_tokenized_filtered]\n",
        "  corpus = [sentence for sentence in corpus if sentence != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WH0VEdXFwS6L"
      },
      "outputs": [],
      "source": [
        "corpus = remove_less_frequente_words(less_frequente_words, sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvdxpqaHzAnp",
        "outputId": "fc52cd10-7bb9-4dc0-d185-31b414139596"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['رستخیز رحمت منتها آتشی افروخته بیشه اندیشه',\n",
              " 'خندان آمدی مفتاح زندان آمدی آمدی بخشش فضل خدا',\n",
              " 'خورشید حاجب تویی امید واجب تویی مطلب تویی طالب تویی منتها',\n",
              " 'سینه اندیشه آراسته حاجت روا',\n",
              " 'روح علم باقی بهانه دغل علت دوا',\n",
              " 'دغل کژ گنه کین مست مست نان شوربا',\n",
              " 'هل عقل هل نان نشاید ماجرا',\n",
              " 'تدبیر رنگ افکنی روم زنگ افکنی جنگ افکنی یری',\n",
              " 'پنهان گوش جان بهانه کسان جان رب زنان والله کیا',\n",
              " 'خامش رفتم پای علم کاغذ بنه بشکن قلم ساقی درآمد الصلا']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "corpus[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHH7-nVDcRAx"
      },
      "source": [
        "## **Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0xOHtNwUOAY1"
      },
      "outputs": [],
      "source": [
        "# tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "eeZyyz09OOBL"
      },
      "outputs": [],
      "source": [
        "with open('tokenizer.h5', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rHUu4qzOWxT",
        "outputId": "0eec5277-eb1b-41b9-8c45-1a1e8a54ed7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('جان', 1),\n",
              " ('دل', 2),\n",
              " ('عشق', 3),\n",
              " ('آب', 4),\n",
              " ('چشم', 5),\n",
              " ('شب', 6),\n",
              " ('جهان', 7),\n",
              " ('شمس', 8),\n",
              " ('دست', 9),\n",
              " ('مست', 10),\n",
              " ('گل', 11),\n",
              " ('یار', 12),\n",
              " ('عقل', 13),\n",
              " ('جمله', 14),\n",
              " ('غم', 15)]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "list(tokenizer.word_index.items())[0:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Jmi-MUXBf4VU"
      },
      "outputs": [],
      "source": [
        " # find more stopwords by sorting the tokenizer word counts\n",
        "#  word_count_sorted = dict(sorted(tokenizer.word_counts.items(), reverse=False, key=lambda t: t[1]))\n",
        "#  word_count_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3ElB_faJPCQs"
      },
      "outputs": [],
      "source": [
        "encoded = tokenizer.texts_to_sequences(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNgPCssskVPV",
        "outputId": "a8ef9e92-872b-4808-ffd6-f1b4942bb8a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2726, 283, 1337, 397, 1610, 967, 143],\n",
              " [185, 568, 1124, 398, 568, 568, 830, 332, 23],\n",
              " [46, 2727, 29, 636, 2031, 29, 831, 29, 258, 29, 1337],\n",
              " [92, 143, 1338, 727, 728],\n",
              " [30, 221, 162, 399, 1611, 968, 244],\n",
              " [1611, 729, 2032, 569, 10, 10, 209, 2728],\n",
              " [464, 13, 464, 209, 1125, 419],\n",
              " [969, 104, 1612, 352, 1126, 1612, 192, 1612, 730],\n",
              " [77, 41, 1, 399, 1613, 1, 259, 245, 284, 1127],\n",
              " [201, 731, 56, 221, 2729, 377, 732, 1128, 31, 570, 186]]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "encoded[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ldaLQ0svKR1B"
      },
      "outputs": [],
      "source": [
        "num_all_words = sum(len(s) for s in encoded) # total number of words in the corpus\n",
        "num_unique_words = len(tokenizer.word_index) + 1  # total number of unique words in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwyN6alDj0Rl",
        "outputId": "c97caa5d-edf9-4b6a-b06c-90c9f46fbd1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32654, 4284)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "num_all_words, num_unique_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generate data**"
      ],
      "metadata": {
        "id": "0a--qhdFXzAh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "iUFMB4csVOSc"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "window_size = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "i7mmQECLjEqz"
      },
      "outputs": [],
      "source": [
        "# CBOW\n",
        "def generate_data(corpus, window_size, num_unique_words):\n",
        "    all_inputs = []\n",
        "    all_outputs = []\n",
        "\n",
        "    for sentence in corpus:\n",
        "        L = len(sentence)\n",
        "        for index, word in enumerate(sentence):\n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "\n",
        "            context_words = []\n",
        "            for i in range(start, end):\n",
        "                if i != index:\n",
        "                    if 0 <= i < L:\n",
        "                        context_words.append(sentence[i])\n",
        "                    else:\n",
        "                        context_words.append(0)\n",
        "            all_inputs.append(context_words)\n",
        "            all_outputs.append(to_categorical(word, num_unique_words))\n",
        "                 \n",
        "    return (np.array(all_inputs), np.array(all_outputs))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-rdJV9GmsAu",
        "outputId": "8cb79dfd-e9fc-416e-d5f2-5a3db8628be6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32654, 4), (32654, 4284))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Create training data\n",
        "X_train, y_train = generate_data(encoded, window_size, num_unique_words)\n",
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQDJ7gZp2yo8",
        "outputId": "33185136-4022-49a6-af5d-911e271ea03e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[   0,    0,  283, 1337],\n",
              "        [   0, 2726, 1337,  397],\n",
              "        [2726,  283,  397, 1610],\n",
              "        ...,\n",
              "        [   8,   22,  619, 1810],\n",
              "        [  22,   24, 1810,    0],\n",
              "        [  24,  619,    0,    0]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "X_train, y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "yxGjSSyJzXu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6ab795-d258-47ef-966c-0758b5a24c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([], dtype=int64),)\n"
          ]
        }
      ],
      "source": [
        "# for i in X_train:\n",
        "#   print(np.where(i == 1))\n",
        "print(np.where(X_train[0] == 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Neural Network**"
      ],
      "metadata": {
        "id": "8mjVK7vmvwiW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7zVNpd6yT7Wx"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_unique_words, output_dim=50, input_length=window_size*2, embeddings_initializer='glorot_uniform'))\n",
        "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(50, )))\n",
        "model.add(Dense(num_unique_words, activation='softmax', kernel_initializer='glorot_uniform'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "YCAYelLXEcXH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi8g7duhbFuV",
        "outputId": "00614a23-62e9-4388-84ad-726947a4bfed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 4, 50)             214200    \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 50)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4284)              218484    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 432,684\n",
            "Trainable params: 432,684\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61aR17ijDIVc",
        "outputId": "f1c9761c-5178-4447-f091-7bd226d933f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 8.1427 - accuracy: 0.0209\n",
            "Epoch 2/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 7.6645 - accuracy: 0.0230\n",
            "Epoch 3/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 7.5777 - accuracy: 0.0230\n",
            "Epoch 4/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 7.5371 - accuracy: 0.0231\n",
            "Epoch 5/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 7.5083 - accuracy: 0.0232\n",
            "Epoch 6/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 7.4830 - accuracy: 0.0231\n",
            "Epoch 7/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 7.4563 - accuracy: 0.0235\n",
            "Epoch 8/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 7.4246 - accuracy: 0.0243\n",
            "Epoch 9/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 7.3861 - accuracy: 0.0269\n",
            "Epoch 10/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 7.3413 - accuracy: 0.0298\n",
            "Epoch 11/300\n",
            "256/256 [==============================] - 7s 27ms/step - loss: 7.2912 - accuracy: 0.0320\n",
            "Epoch 12/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 7.2363 - accuracy: 0.0336\n",
            "Epoch 13/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 7.1770 - accuracy: 0.0362\n",
            "Epoch 14/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 7.1121 - accuracy: 0.0393\n",
            "Epoch 15/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 7.0409 - accuracy: 0.0439\n",
            "Epoch 16/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 6.9619 - accuracy: 0.0484\n",
            "Epoch 17/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 6.8756 - accuracy: 0.0528\n",
            "Epoch 18/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 6.7823 - accuracy: 0.0563\n",
            "Epoch 19/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 6.6825 - accuracy: 0.0626\n",
            "Epoch 20/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 6.5779 - accuracy: 0.0665\n",
            "Epoch 21/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 6.4695 - accuracy: 0.0722\n",
            "Epoch 22/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 6.3584 - accuracy: 0.0761\n",
            "Epoch 23/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 6.2454 - accuracy: 0.0824\n",
            "Epoch 24/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 6.1315 - accuracy: 0.0867\n",
            "Epoch 25/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 6.0171 - accuracy: 0.0916\n",
            "Epoch 26/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 5.9032 - accuracy: 0.0967\n",
            "Epoch 27/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 5.7900 - accuracy: 0.1017\n",
            "Epoch 28/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 5.6782 - accuracy: 0.1068\n",
            "Epoch 29/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 5.5679 - accuracy: 0.1113\n",
            "Epoch 30/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 5.4601 - accuracy: 0.1155\n",
            "Epoch 31/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 5.3546 - accuracy: 0.1204\n",
            "Epoch 32/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 5.2516 - accuracy: 0.1249\n",
            "Epoch 33/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 5.1509 - accuracy: 0.1298\n",
            "Epoch 34/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 5.0529 - accuracy: 0.1339\n",
            "Epoch 35/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 4.9575 - accuracy: 0.1403\n",
            "Epoch 36/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 4.8645 - accuracy: 0.1458\n",
            "Epoch 37/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 4.7739 - accuracy: 0.1496\n",
            "Epoch 38/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 4.6858 - accuracy: 0.1558\n",
            "Epoch 39/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 4.5999 - accuracy: 0.1609\n",
            "Epoch 40/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 4.5165 - accuracy: 0.1656\n",
            "Epoch 41/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 4.4353 - accuracy: 0.1728\n",
            "Epoch 42/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 4.3563 - accuracy: 0.1788\n",
            "Epoch 43/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 4.2794 - accuracy: 0.1849\n",
            "Epoch 44/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 4.2045 - accuracy: 0.1916\n",
            "Epoch 45/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 4.1317 - accuracy: 0.1969\n",
            "Epoch 46/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 4.0607 - accuracy: 0.2035\n",
            "Epoch 47/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 3.9915 - accuracy: 0.2102\n",
            "Epoch 48/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 3.9243 - accuracy: 0.2169\n",
            "Epoch 49/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 3.8582 - accuracy: 0.2243\n",
            "Epoch 50/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 3.7943 - accuracy: 0.2302\n",
            "Epoch 51/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 3.7315 - accuracy: 0.2399\n",
            "Epoch 52/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 3.6704 - accuracy: 0.2464\n",
            "Epoch 53/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 3.6103 - accuracy: 0.2549\n",
            "Epoch 54/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 3.5518 - accuracy: 0.2637\n",
            "Epoch 55/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 3.4944 - accuracy: 0.2720\n",
            "Epoch 56/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 3.4382 - accuracy: 0.2820\n",
            "Epoch 57/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 3.3828 - accuracy: 0.2907\n",
            "Epoch 58/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 3.3290 - accuracy: 0.3005\n",
            "Epoch 59/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 3.2758 - accuracy: 0.3098\n",
            "Epoch 60/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 3.2237 - accuracy: 0.3177\n",
            "Epoch 61/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 3.1727 - accuracy: 0.3290\n",
            "Epoch 62/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 3.1224 - accuracy: 0.3387\n",
            "Epoch 63/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 3.0728 - accuracy: 0.3487\n",
            "Epoch 64/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 3.0245 - accuracy: 0.3586\n",
            "Epoch 65/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.9766 - accuracy: 0.3684\n",
            "Epoch 66/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.9297 - accuracy: 0.3776\n",
            "Epoch 67/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.8834 - accuracy: 0.3880\n",
            "Epoch 68/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.8380 - accuracy: 0.3976\n",
            "Epoch 69/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.7933 - accuracy: 0.4076\n",
            "Epoch 70/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.7495 - accuracy: 0.4188\n",
            "Epoch 71/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.7061 - accuracy: 0.4283\n",
            "Epoch 72/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.6639 - accuracy: 0.4380\n",
            "Epoch 73/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 2.6220 - accuracy: 0.4475\n",
            "Epoch 74/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 2.5811 - accuracy: 0.4576\n",
            "Epoch 75/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.5407 - accuracy: 0.4669\n",
            "Epoch 76/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.5011 - accuracy: 0.4759\n",
            "Epoch 77/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 2.4621 - accuracy: 0.4850\n",
            "Epoch 78/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.4235 - accuracy: 0.4949\n",
            "Epoch 79/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.3861 - accuracy: 0.5035\n",
            "Epoch 80/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.3490 - accuracy: 0.5121\n",
            "Epoch 81/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.3126 - accuracy: 0.5212\n",
            "Epoch 82/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.2771 - accuracy: 0.5295\n",
            "Epoch 83/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.2420 - accuracy: 0.5386\n",
            "Epoch 84/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.2076 - accuracy: 0.5448\n",
            "Epoch 85/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.1737 - accuracy: 0.5532\n",
            "Epoch 86/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 2.1404 - accuracy: 0.5625\n",
            "Epoch 87/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 2.1082 - accuracy: 0.5691\n",
            "Epoch 88/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.0759 - accuracy: 0.5768\n",
            "Epoch 89/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 2.0446 - accuracy: 0.5843\n",
            "Epoch 90/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 2.0137 - accuracy: 0.5918\n",
            "Epoch 91/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.9836 - accuracy: 0.5986\n",
            "Epoch 92/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.9540 - accuracy: 0.6043\n",
            "Epoch 93/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.9248 - accuracy: 0.6114\n",
            "Epoch 94/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.8963 - accuracy: 0.6174\n",
            "Epoch 95/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.8682 - accuracy: 0.6245\n",
            "Epoch 96/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.8409 - accuracy: 0.6302\n",
            "Epoch 97/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.8140 - accuracy: 0.6358\n",
            "Epoch 98/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.7875 - accuracy: 0.6428\n",
            "Epoch 99/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.7615 - accuracy: 0.6484\n",
            "Epoch 100/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.7361 - accuracy: 0.6538\n",
            "Epoch 101/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.7113 - accuracy: 0.6604\n",
            "Epoch 102/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.6867 - accuracy: 0.6652\n",
            "Epoch 103/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.6628 - accuracy: 0.6705\n",
            "Epoch 104/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.6392 - accuracy: 0.6761\n",
            "Epoch 105/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.6161 - accuracy: 0.6808\n",
            "Epoch 106/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.5937 - accuracy: 0.6852\n",
            "Epoch 107/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.5713 - accuracy: 0.6908\n",
            "Epoch 108/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.5494 - accuracy: 0.6954\n",
            "Epoch 109/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.5282 - accuracy: 0.6995\n",
            "Epoch 110/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.5073 - accuracy: 0.7039\n",
            "Epoch 111/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.4869 - accuracy: 0.7070\n",
            "Epoch 112/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.4667 - accuracy: 0.7111\n",
            "Epoch 113/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.4470 - accuracy: 0.7167\n",
            "Epoch 114/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.4276 - accuracy: 0.7193\n",
            "Epoch 115/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.4084 - accuracy: 0.7244\n",
            "Epoch 116/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.3898 - accuracy: 0.7275\n",
            "Epoch 117/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.3716 - accuracy: 0.7304\n",
            "Epoch 118/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.3537 - accuracy: 0.7351\n",
            "Epoch 119/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.3360 - accuracy: 0.7394\n",
            "Epoch 120/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.3187 - accuracy: 0.7429\n",
            "Epoch 121/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.3016 - accuracy: 0.7456\n",
            "Epoch 122/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.2849 - accuracy: 0.7496\n",
            "Epoch 123/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.2687 - accuracy: 0.7530\n",
            "Epoch 124/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.2525 - accuracy: 0.7560\n",
            "Epoch 125/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.2370 - accuracy: 0.7587\n",
            "Epoch 126/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.2215 - accuracy: 0.7616\n",
            "Epoch 127/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.2062 - accuracy: 0.7648\n",
            "Epoch 128/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.1913 - accuracy: 0.7671\n",
            "Epoch 129/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.1767 - accuracy: 0.7705\n",
            "Epoch 130/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.1624 - accuracy: 0.7736\n",
            "Epoch 131/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.1481 - accuracy: 0.7765\n",
            "Epoch 132/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.1344 - accuracy: 0.7803\n",
            "Epoch 133/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.1207 - accuracy: 0.7822\n",
            "Epoch 134/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.1074 - accuracy: 0.7852\n",
            "Epoch 135/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.0944 - accuracy: 0.7864\n",
            "Epoch 136/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.0815 - accuracy: 0.7908\n",
            "Epoch 137/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.0688 - accuracy: 0.7920\n",
            "Epoch 138/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.0563 - accuracy: 0.7947\n",
            "Epoch 139/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.0441 - accuracy: 0.7967\n",
            "Epoch 140/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.0320 - accuracy: 0.8000\n",
            "Epoch 141/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 1.0201 - accuracy: 0.8016\n",
            "Epoch 142/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 1.0085 - accuracy: 0.8039\n",
            "Epoch 143/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.9971 - accuracy: 0.8068\n",
            "Epoch 144/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.9859 - accuracy: 0.8080\n",
            "Epoch 145/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.9749 - accuracy: 0.8104\n",
            "Epoch 146/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.9642 - accuracy: 0.8122\n",
            "Epoch 147/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.9536 - accuracy: 0.8137\n",
            "Epoch 148/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.9429 - accuracy: 0.8171\n",
            "Epoch 149/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.9327 - accuracy: 0.8173\n",
            "Epoch 150/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.9224 - accuracy: 0.8183\n",
            "Epoch 151/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.9126 - accuracy: 0.8210\n",
            "Epoch 152/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.9028 - accuracy: 0.8222\n",
            "Epoch 153/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.8931 - accuracy: 0.8240\n",
            "Epoch 154/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.8837 - accuracy: 0.8258\n",
            "Epoch 155/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.8742 - accuracy: 0.8269\n",
            "Epoch 156/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.8651 - accuracy: 0.8284\n",
            "Epoch 157/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.8559 - accuracy: 0.8303\n",
            "Epoch 158/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.8470 - accuracy: 0.8317\n",
            "Epoch 159/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.8383 - accuracy: 0.8341\n",
            "Epoch 160/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.8298 - accuracy: 0.8340\n",
            "Epoch 161/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.8214 - accuracy: 0.8368\n",
            "Epoch 162/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.8128 - accuracy: 0.8385\n",
            "Epoch 163/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.8046 - accuracy: 0.8411\n",
            "Epoch 164/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.7965 - accuracy: 0.8405\n",
            "Epoch 165/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.7885 - accuracy: 0.8421\n",
            "Epoch 166/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.7806 - accuracy: 0.8439\n",
            "Epoch 167/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.7729 - accuracy: 0.8450\n",
            "Epoch 168/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.7653 - accuracy: 0.8463\n",
            "Epoch 169/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.7579 - accuracy: 0.8474\n",
            "Epoch 170/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.7506 - accuracy: 0.8491\n",
            "Epoch 171/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.7432 - accuracy: 0.8493\n",
            "Epoch 172/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.7360 - accuracy: 0.8514\n",
            "Epoch 173/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.7289 - accuracy: 0.8522\n",
            "Epoch 174/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.7220 - accuracy: 0.8541\n",
            "Epoch 175/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.7151 - accuracy: 0.8553\n",
            "Epoch 176/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.7082 - accuracy: 0.8563\n",
            "Epoch 177/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.7017 - accuracy: 0.8567\n",
            "Epoch 178/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.6951 - accuracy: 0.8587\n",
            "Epoch 179/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.6887 - accuracy: 0.8590\n",
            "Epoch 180/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.6824 - accuracy: 0.8592\n",
            "Epoch 181/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.6759 - accuracy: 0.8619\n",
            "Epoch 182/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.6697 - accuracy: 0.8625\n",
            "Epoch 183/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.6637 - accuracy: 0.8635\n",
            "Epoch 184/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.6576 - accuracy: 0.8643\n",
            "Epoch 185/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.6519 - accuracy: 0.8650\n",
            "Epoch 186/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.6458 - accuracy: 0.8659\n",
            "Epoch 187/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.6401 - accuracy: 0.8668\n",
            "Epoch 188/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.6343 - accuracy: 0.8678\n",
            "Epoch 189/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.6288 - accuracy: 0.8682\n",
            "Epoch 190/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.6233 - accuracy: 0.8709\n",
            "Epoch 191/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.6177 - accuracy: 0.8711\n",
            "Epoch 192/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.6125 - accuracy: 0.8721\n",
            "Epoch 193/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.6072 - accuracy: 0.8709\n",
            "Epoch 194/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.6018 - accuracy: 0.8734\n",
            "Epoch 195/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.5967 - accuracy: 0.8755\n",
            "Epoch 196/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.5917 - accuracy: 0.8752\n",
            "Epoch 197/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.5865 - accuracy: 0.8766\n",
            "Epoch 198/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.5817 - accuracy: 0.8772\n",
            "Epoch 199/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.5767 - accuracy: 0.8782\n",
            "Epoch 200/300\n",
            "256/256 [==============================] - 5s 20ms/step - loss: 0.5720 - accuracy: 0.8784\n",
            "Epoch 201/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.5671 - accuracy: 0.8801\n",
            "Epoch 202/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.5625 - accuracy: 0.8810\n",
            "Epoch 203/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.5577 - accuracy: 0.8819\n",
            "Epoch 204/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.5532 - accuracy: 0.8817\n",
            "Epoch 205/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.5488 - accuracy: 0.8837\n",
            "Epoch 206/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.5441 - accuracy: 0.8833\n",
            "Epoch 207/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.5398 - accuracy: 0.8837\n",
            "Epoch 208/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.5353 - accuracy: 0.8858\n",
            "Epoch 209/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.5312 - accuracy: 0.8862\n",
            "Epoch 210/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.5270 - accuracy: 0.8876\n",
            "Epoch 211/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.5228 - accuracy: 0.8874\n",
            "Epoch 212/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.5186 - accuracy: 0.8890\n",
            "Epoch 213/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.5146 - accuracy: 0.8898\n",
            "Epoch 214/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.5106 - accuracy: 0.8895\n",
            "Epoch 215/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.5066 - accuracy: 0.8916\n",
            "Epoch 216/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.5028 - accuracy: 0.8912\n",
            "Epoch 217/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4988 - accuracy: 0.8915\n",
            "Epoch 218/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4951 - accuracy: 0.8933\n",
            "Epoch 219/300\n",
            "256/256 [==============================] - 5s 20ms/step - loss: 0.4912 - accuracy: 0.8939\n",
            "Epoch 220/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4875 - accuracy: 0.8948\n",
            "Epoch 221/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4837 - accuracy: 0.8953\n",
            "Epoch 222/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4801 - accuracy: 0.8957\n",
            "Epoch 223/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4765 - accuracy: 0.8962\n",
            "Epoch 224/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4732 - accuracy: 0.8956\n",
            "Epoch 225/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4695 - accuracy: 0.8975\n",
            "Epoch 226/300\n",
            "256/256 [==============================] - 5s 20ms/step - loss: 0.4660 - accuracy: 0.8977\n",
            "Epoch 227/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4626 - accuracy: 0.8982\n",
            "Epoch 228/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4591 - accuracy: 0.8992\n",
            "Epoch 229/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4560 - accuracy: 0.8989\n",
            "Epoch 230/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4526 - accuracy: 0.9001\n",
            "Epoch 231/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4493 - accuracy: 0.9013\n",
            "Epoch 232/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4460 - accuracy: 0.9009\n",
            "Epoch 233/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4429 - accuracy: 0.9017\n",
            "Epoch 234/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4399 - accuracy: 0.9017\n",
            "Epoch 235/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4367 - accuracy: 0.9019\n",
            "Epoch 236/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4336 - accuracy: 0.9025\n",
            "Epoch 237/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4307 - accuracy: 0.9032\n",
            "Epoch 238/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4276 - accuracy: 0.9030\n",
            "Epoch 239/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4247 - accuracy: 0.9043\n",
            "Epoch 240/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4218 - accuracy: 0.9043\n",
            "Epoch 241/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4187 - accuracy: 0.9052\n",
            "Epoch 242/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4161 - accuracy: 0.9054\n",
            "Epoch 243/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4132 - accuracy: 0.9057\n",
            "Epoch 244/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4104 - accuracy: 0.9071\n",
            "Epoch 245/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4076 - accuracy: 0.9071\n",
            "Epoch 246/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.4049 - accuracy: 0.9071\n",
            "Epoch 247/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.4020 - accuracy: 0.9079\n",
            "Epoch 248/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.3995 - accuracy: 0.9081\n",
            "Epoch 249/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3967 - accuracy: 0.9090\n",
            "Epoch 250/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3943 - accuracy: 0.9090\n",
            "Epoch 251/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3918 - accuracy: 0.9095\n",
            "Epoch 252/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.3893 - accuracy: 0.9098\n",
            "Epoch 253/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3867 - accuracy: 0.9105\n",
            "Epoch 254/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.3842 - accuracy: 0.9106\n",
            "Epoch 255/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3817 - accuracy: 0.9109\n",
            "Epoch 256/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3794 - accuracy: 0.9114\n",
            "Epoch 257/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3770 - accuracy: 0.9113\n",
            "Epoch 258/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3746 - accuracy: 0.9115\n",
            "Epoch 259/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.3722 - accuracy: 0.9121\n",
            "Epoch 260/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3697 - accuracy: 0.9123\n",
            "Epoch 261/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3675 - accuracy: 0.9130\n",
            "Epoch 262/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3653 - accuracy: 0.9131\n",
            "Epoch 263/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3632 - accuracy: 0.9126\n",
            "Epoch 264/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.3608 - accuracy: 0.9123\n",
            "Epoch 265/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3585 - accuracy: 0.9145\n",
            "Epoch 266/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.3566 - accuracy: 0.9151\n",
            "Epoch 267/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.3544 - accuracy: 0.9141\n",
            "Epoch 268/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3522 - accuracy: 0.9148\n",
            "Epoch 269/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3502 - accuracy: 0.9158\n",
            "Epoch 270/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.3478 - accuracy: 0.9156\n",
            "Epoch 271/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.3460 - accuracy: 0.9156\n",
            "Epoch 272/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3439 - accuracy: 0.9157\n",
            "Epoch 273/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.3419 - accuracy: 0.9167\n",
            "Epoch 274/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3399 - accuracy: 0.9172\n",
            "Epoch 275/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3379 - accuracy: 0.9172\n",
            "Epoch 276/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3361 - accuracy: 0.9175\n",
            "Epoch 277/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.3340 - accuracy: 0.9184\n",
            "Epoch 278/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3322 - accuracy: 0.9187\n",
            "Epoch 279/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3303 - accuracy: 0.9182\n",
            "Epoch 280/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3284 - accuracy: 0.9185\n",
            "Epoch 281/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3265 - accuracy: 0.9191\n",
            "Epoch 282/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3248 - accuracy: 0.9189\n",
            "Epoch 283/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3230 - accuracy: 0.9191\n",
            "Epoch 284/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.3211 - accuracy: 0.9200\n",
            "Epoch 285/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3196 - accuracy: 0.9196\n",
            "Epoch 286/300\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.3176 - accuracy: 0.9203\n",
            "Epoch 287/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3158 - accuracy: 0.9212\n",
            "Epoch 288/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3142 - accuracy: 0.9206\n",
            "Epoch 289/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3125 - accuracy: 0.9204\n",
            "Epoch 290/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3109 - accuracy: 0.9202\n",
            "Epoch 291/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3092 - accuracy: 0.9206\n",
            "Epoch 292/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3076 - accuracy: 0.9217\n",
            "Epoch 293/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3059 - accuracy: 0.9206\n",
            "Epoch 294/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3042 - accuracy: 0.9215\n",
            "Epoch 295/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3027 - accuracy: 0.9217\n",
            "Epoch 296/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.3012 - accuracy: 0.9226\n",
            "Epoch 297/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.2997 - accuracy: 0.9226\n",
            "Epoch 298/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.2981 - accuracy: 0.9224\n",
            "Epoch 299/300\n",
            "256/256 [==============================] - 5s 19ms/step - loss: 0.2965 - accuracy: 0.9220\n",
            "Epoch 300/300\n",
            "256/256 [==============================] - 5s 21ms/step - loss: 0.2951 - accuracy: 0.9231\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train, y_train, epochs=300, verbose=1, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loss and Accuracy plot**"
      ],
      "metadata": {
        "id": "WteXo7Xr21h3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "-WycnX3zl3Tx",
        "outputId": "674dacc3-d6ef-49fe-e90a-7e7db6dfcf36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7feb6b2b5210>]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['loss'], color='green', label='Loss')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "iwDDlk79fYSe"
      },
      "outputs": [],
      "source": [
        "model.save('word2vec.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Model**"
      ],
      "metadata": {
        "id": "P4GvDkAcvmdg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "McWk2c7kfoOd"
      },
      "outputs": [],
      "source": [
        "# laod model\n",
        "model = load_model('word2vec.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0JiFkSzhi52",
        "outputId": "7fb612c0-86c3-4607-ff2d-45f06144353c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7feb6b270150>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Mu08LLKqbUjL"
      },
      "outputs": [],
      "source": [
        "weights = model.get_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT7U6Sw6xYql",
        "outputId": "9a887c47-4083-426a-fc29-ffdab8434e07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.8212957 ,  0.30798614, -1.5770972 ,  0.6195138 ,  0.46113226,\n",
              "       -1.0850326 , -1.5585592 ,  0.6005408 ,  0.8289619 , -1.4800417 ,\n",
              "       -0.49123228, -0.7261426 , -0.15095983,  0.2454091 , -0.04197242,\n",
              "        0.6444775 , -0.6405439 ,  0.7550058 ,  0.83351177,  1.0177284 ,\n",
              "        0.30975813, -2.5754516 , -0.18166237, -1.6441044 ,  0.18483934,\n",
              "       -3.3331501 , -0.36649308,  3.0233157 ,  2.4002042 ,  0.37174973,\n",
              "        1.3382609 , -1.5898685 ,  1.8035638 , -2.5048563 , -0.02682108,\n",
              "       -0.56691843, -0.7731482 , -0.21949543,  0.93925315,  2.597449  ,\n",
              "        2.4126701 , -1.4621407 ,  1.4653767 , -0.31707886,  2.5772147 ,\n",
              "        0.62155014, -1.970795  , -2.4434478 ,  1.9304849 , -0.05640695],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "weights[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "p4qSGi4xIBc3"
      },
      "outputs": [],
      "source": [
        "with open('tokenizer.h5', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict([[696,697,699,700]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfiKiOEbbVnQ",
        "outputId": "70401d2b-42b5-4056-feb0-3c7432019d04"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.7305899e-18, 2.9325605e-07, 9.7689037e-05, ..., 3.9029084e-12,\n",
              "        7.8051662e-12, 1.0194475e-17]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "word2vec_cbow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}